---
layout: post
title: Significance testing for binary classification
scribble: "true"
---

# Significance testing for classifiers

## Comparing two binary classifiers based on individual samples

### Given

- Two binary classifiers, `A` and `B`.
- The evaluation of `A` and `B` on each data point of a test set `T`.
- Example

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<tbody>
  <tr>
    <td class="tg-0lax">True label</td>
    <td class="tg-0lax">A's prediction</td>
    <td class="tg-0lax">B's Prediction</td>
  </tr>
  <tr>
    <td class="tg-0lax">1</td>
    <td class="tg-0lax">1</td>
    <td class="tg-0lax">1</td>
  </tr>
  <tr>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">1</td>
  </tr>
  <tr>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
  </tr>
  <tr>
    <td class="tg-0lax">1</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">1</td>
  </tr>
</tbody>
</table>
### Question
Is `A` better than `B`?

### Intuition

Let us create a confusion matrix:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tbody>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">B right</td>
    <td class="tg-0pky">B wrong</td>
  </tr>
  <tr>
    <td class="tg-0pky">A right</td>
    <td class="tg-0pky">AB</td>
    <td class="tg-0pky">AB'</td>
  </tr>
  <tr>
    <td class="tg-0pky">A wrong</td>
    <td class="tg-0pky">A'B</td>
    <td class="tg-0pky">A'B'</td>
  </tr>
</tbody>
</table>

- Intuitively, the points that should help us in deciding if classifier `A` truly improves over classifier `B` are `AB'` and `A'B`. Specifically, we want `A'B << AB'`.

- Let `n = A'B + AB'`, and `k = AB'`.

- The _null hypothesis_ is that both `A` and `B` have about the same performance. That is, we iterate over over test set, and with a probability of `50%` we pick either `A` or `B` as the winner.

- In such cases, `A'B ~ AB'` or `k = AB' = AB' = 0.5n`.

### Evaluation

- First, let's assume that we have $$>12$$ data points, and approximate the binomial $$(np, np(1 - p))$$ with $$\mathcal{N}(np, np(1 - p))$$.

- **If** our null hypothesis is true, the probability of observing the `k` that we are indeed observing, or less would be: $$p(x \leq k)$$. $$1 - p(x \leq k)$$ gives us the probability that this k (or higher) is possible under the null hypothesis; the so-called p-value.

#### Calculating $$p(x \leq k)$$

- We calculate this number by first computing the z-score. That gives us the number of standard deviations that we are away from the mean of a normal distribution $$N(np, np(1 - p))$$ if we observe k. That is given by:
  $$z = \frac{k - 0.5n}{0.5\sqrt{n}}$$

- Substitute this `z` in a standard normal table to get `p(x \leq k)`. 

- If the `p-value` is lower than `0.01`, you say that the null hypothesis is rejected with `p = 0.01`.

```py
import scipy.stats as st
from collections import Counter

def micro_sign_test(a, b):
    # a_i = {0, 1}, b_i = {0, 1}
    # 0/1 -> output is incorrect/correct
    res = []
    for a_i, b_i in zip(a, b):
        res.append((a_i, b_i))

    counts = Counter(res)

    n = counts[(1, 0)] + counts[(0, 1)]  # is the number of times that ai and bi differ
    k = counts[(1, 0)] # number of times that ai is larger than bi
    z = (k - 0.5*n) / (0.5 * math.sqrt(n))
    p = 1 - st.norm.cdf(z)
    return z, p
```


# Significance test for two means

- Suppose we are given two means, ma and mb.
- Consider the distribution of the difference between the two means, and assume that the distribution N(0, 1).
- Calculate the z-score as:
  z = (ma - mb) / sqrt(std1**2 / n1 + std2**2 / n2)

- As usual, z score lookup in the table gives you the cumulative probability covered by that z-score. A high z-score means that you are
- quite far from the 0.

### Null hypothesis: the two means are the same
- Consider a distribution of the difference between the two means.
- https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html
- 