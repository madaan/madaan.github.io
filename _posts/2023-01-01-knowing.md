---
layout: post
title: Knowing when 
root: "../"
blog: "true"
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

Language models like ChatGPT are often confidently wrong. Apart from making for some gotcha Tweets, this raises an important question: how do we know if these models _really_ understand what they are talking about? LM researchers have obviously been thinking about this problem for a while, for example, see [this](https://arxiv.org/abs/2012.00955) and [this](https://arxiv.org/abs/2207.05221).
This is a valid concern, and is one of the core limitations that will need to be addressed before we can use these models in real-world applications.

The thing is, language models are not special when it comes to being completely wrong with total confidence. Humans also routinely perform non-trivial tasks without a deep understanding of what they are working on. As a simple example, consider the problem of solving a system of linear equations. Say we train a student, STUDENT_A, to solve the following system of equations:

## Training

$$
\begin{aligned}
x + y &= 4 \\
2x + y &= 8
\end{aligned}
$$


Middle-school students are often given algorithmic instructions to help them arrive at the solution.

* `Step 1:` For the first equation, collect all the terms of y on one side of the equation:

$$
\begin{aligned}
y &= 4 - x
\end{aligned}
$$

* `Step 2:` Substitute the value of y in the second equation, solve for x

$$
\begin{aligned}
2x + (4 - x) &= 8 \\
x &= 4
\end{aligned}
$$

* Step 3: Substitute the value of y=x in the first equation, solve for y
  
    $$y = 4 - 4 = 0$$



## Testing

STUDENT_A has perfected this algorithm, and successfully solves many systems of equations with two variables. 

Until one day, STUDENT_A is given the following pair of equations to solve:

$$
\begin{aligned}
x + y &= 4 \\
2x + 2y &= 8
\end{aligned}
$$

STUDENT_A starts following the algorithm::

* `Step 1:` For the first equation, collect all the terms of y on one side of the equation

    $$y = 4 - x$$

* `Step 2:` Substitute the value of y in the second equation, solve for x

$$
\begin{aligned}
2x + 2(4 - x) &= 8 \\
2x + 8 - 2x &= 8 \\
\implies 8 &= 8 
\end{aligned}
$$

Umm what!?. Panic. This was *not* a part of the plan. Wasn’t the method supposed to yield `x = some number?` 

Maybe there’s a mistake. STUDENT_A repeats step 2 again. Nothing. They try to start from scratch by going back to step 1. Nope. Then they repeat this process a few times…of course nothing works.


At this point, our student is stuck.


The algorithm cannot be wrong! It’s _all_ they were trained to do, and it has worked for them so far. STUDENT_A is sure that they are making a silly mistake somewhere that they are too panicked to see. The question **has to be** answered though, so they try to 'guess' the correct answer. Surely, the answer cannot be something simple, like 0 or 4, otherwise they would have seen it? Perhaps it's a nasty number. 19 sounds like a nasty yet plausible number. So our student kind of ignores step 2 and moves to step 3:

* `Step 3:` Substitute the value of y=x in the first equation, solve for y

    $$y = 4 - 19 = -15$$

So STUDENT_A generates `x = 19` and `y = -15`. They even put the numbers back in the first equation and everything checks out: 19 - 15 = 4! STUDENT_A moves on to the next equation, taking pride in how good they’re getting at this stuff (even their intuitions are correct).

Is the answer correct? **Yes.** Is the answer correct for the right reasons? No.


## How can we know if a model understands the problem?

The above made up example highlights how evaluating confidence might not be enough to determine if a model really understands the problem, and that probing and explanations might be necessary. Some elaboration on this:

* Evaluating Explanations: We can't prove that student A doesn't understand the problem. After all, they did give us something that's correct. It turns out that the fact that the problem had multiple solutions played in their favor. This often happens in real-world applications of language generation where the tasks (write a poem, change style of a text, summarize, etc.) have multiple solutions.

* Confidence: Measuring the confidence of the model in its answer could be one way to determine if the model is just memorizing patterns or actually understands the problem. However, just like Student A was confident in their answer after they checked it, the model might be confident in its answer even if it doesn't understand the problem.

* Probing: Let’s consider another student, B, who understood the purpose of solving two equations. Different from A, who understands the algorithm, Perhaps B understands that the two equations are two 2 constraints, representing properties of some systems. Such a student might be able to recognize that both the equations (x + y = 4, 2x + 2y = 8) are just one equation, x + y = 4, and specify that any pairs (x, y) where x + y = 4 will work (including x = 19, y = -15). This indicates that asking for an explanation in addition to the answer could be a crucial step in determining if a model really understands the problem.


The example also highlights that there is great value in training models that work most of the time. Student A is able to solve many problems, and is able to do so with a simple algorithm. This might be sufficient for many applications.



[1] 
Substituting smaller numbers with larger numbers and with greek alphabets allows us to disentangle pattern matching vs. a more profound problem-solving ability. Our results (already approved and published on arxiv) show that it is more about the former when it comes to language models. We want to stress that algorithmic understanding is critical for many domains and can lead to breakthroughs in many fields.
