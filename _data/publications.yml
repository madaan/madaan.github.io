
- title: "FLOWGEN: Fast and slow graph generation"
  year: 2022
  authors: "Aman Madaan and Yiming Yang"
  conference: "Dynamic Neural Networks Workshop at ICML 2022"
  org: Dynn@ICML
  abstract: "Machine learning models typically exert the same computational power on both easy and challenging examples. This is in stark contrast with humans, who use either fast (instinctive) or slow (analytical) thinking depending on the problem difficulty, a property called the dual-process theory of mind. We present FLOWGEN, a graph-generation model inspired by the dual-process theory of mind that generates large graphs incrementally. Depending on the difficulty of completing the graph at the current step, graph generation is routed to either a fast (weaker) or a slow (stronger) model. The fast and slow models have identical architectures, but vary in the number of parameters and consequently the strength. Experiments on real-world graphs show that ours can successfully generate graphs similar to those generated by a single large model in a fraction of time."
  pdf: "https://arxiv.org/pdf/2207.07656.pdf"

- title: "Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback"
  year: 2022
  authors: "Niket Tandon*, Aman Madaan*, Peter Clark, and Yiming Yang"
  conference: "NAACL 2022 (Findings)"
  org: NAACL
  abstract: "Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for script generation, that takes a goal (e.g., 'bake a cake') and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, FBNet, learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility."
  pdf: "https://arxiv.org/pdf/2112.09737.pdf"
  code: https://github.com/allenai/interscript


- title: "Memory-assisted prompt editing to improve GPT-3 after deployment"
  year: 2022
  authors: "Aman Madaan*, Niket Tandon*, Peter Clark, and Yiming Yang"
  conference: "Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022"
  org: CSRR@ACL
  code: https://github.com/madaan/memprompt
  abstract: "Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret 'What word is similar to good?' to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two  advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs."
  pdf: "https://arxiv.org/pdf/2201.06009.pdf"

- title: "CURIE: An Iterative Querying Approach for Reasoning About Situations" 
  authors: "Aman Madaan*, Dheeraj Rajagopal*, Yiming Yang, Abhilasha Ravichander, Eduard Hovy, and Shrimai Prabhumoye."
  conference: "Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022"
  year: 2022
  pdf: "https://aclanthology.org/2022.csrr-1.7.pdf"
  abstract: "Recently, models have been shown to predict the effects of unexpected situations, e.g., would cloudy skies help or hinder plant growth? Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context. We propose a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st-graph) using natural language queries over a finetuned language model (M). Across multiple domains, CURIE generates st-graphs that humans find relevant and meaningful in eliciting the consequences of a new situation. We show that st-graphs generated by CURIE improve a situational reasoning end task (WIQA-QA) by 3 points on accuracy by simply augmenting their input with our generated situational graphs, especially for a hard subset that requires background knowledge and multi-hop reasoning."
  org: CSRR@ACL
  code: https://github.com/dheerajrajagopal/EIGEN

- title: "Think about it! Improving defeasible reasoning by first modeling the question scenario"
  authors: "Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy"
  conference: "EMNLP 2021"
  year: 2021
  organization: '<a href="https://2021.emnlp.org//>EMNLP</a>' 
  pdf: "https://arxiv.org/pdf/2110.12349.pdf" 
  abstract: "Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to 'think about' a question and explicitly model the scenario, rather than answering reflexively."
  org: EMNLP
  code: https://github.com/madaan/thinkaboutit

- title: "Could you give me a hint? Generating inference graphs for defeasible reasoning"
  authors: "Aman Madaan*, Dheeraj Rajagopal*, Niket Tandon*, Yiming Yang, and Eduard Hovy"
  conference: "ACL 2021 (Findings)"
  year: 2021
  organization: '<a href="https://2021.aclweb.org//>ACL</a>'
  pdf: "https://aclanthology.org/2021.findings-acl.456.pdf"
  talk: "https://madaan.github.io/res/presentations/cygmah.pdf"
  abstract: "Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from another NLP task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 20% by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning."
  org: ACL
  code: https://github.com/madaan/defeasible_graphs

- title: "Neural language modeling for contextualized temporal graph generation"
  authors: "Aman Madaan and Yiming Yang"
  conference: "NAACL 2021"
  year: 2021
  organization: '<a href="https://2021.naacl.org/>NAACL</a>'
  pdf: "https://www.aclweb.org/anthology/2021.naacl-main.67.pdf"
  talk: "https://madaan.github.io/res/presentations/naacl_temporal_gen.pdf"
  tldr: "https://madaan.github.io/res/tldr/graph_gen_tldr.jpg"
  abstract: "This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. Code and pre-trained models are available at https://github.com/madaan/temporal-graph-gen."
  org: NAACL
  code: https://github.com/madaan/temporal-graph-gen



- title: "Politeness Transfer: A Tag and Generate Approach" 
  authors: "Aman Madaan*, Amrith Setlur*, Tanmay Parekh*, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, and Shrimai Prabhumoye."
  conference: "ACL 2020"
  year: 2020
  pdf: "https://www.aclweb.org/anthology/2020.acl-main.169.pdf"
  talk: "https://madaan.github.io/res/presentations/tag_and_generate.pdf"
  abstract: "This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate."
  org: ACL
  code: https://github.com/tag-and-generate

- title: "Practical Comparable Data Collection for Low-Resource Languages via Images" 
  authors: "Aman Madaan, Shruti Rijhwani, Antonios Anastasopoulos, Yiming Yang, and Graham Neubig"
  conference: "Proceedings of the Practical ML for Developing Countries Workshop, ICLR 2020"
  year: 2020
  pdf: "https://arxiv.org/pdf/2004.11954.pdf"
  abstract: "We propose a method of curating high-quality comparable training data for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the source and target languages by getting captions for such images in both languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected through our approach by experimenting on two downstream tasks - machine translation and dictionary extraction. All code and data are available at https://github.com/madaan/PML4DC-Comparable-Data-Collection"
  talk: "https://pml4dc.github.io/iclr2020/pdf/PML4DC2020_27.pdf"
  org: PML4DC@ICLR
  code: https://github.com/madaan/PML4DC-Comparable-Data-Collection

- title: "Numerical Relation Extraction with Minimal Supervision"
  authors: "Aman Madaan, Ashish Mittal, Mausam, Ganesh Ramakrishnan, and Sunita Sarawagi"
  conference: "AAAI 2016"
  year: 2016
  organization: '<a href="https://www.aaai.org/">AAAI</a>'
  pdf: "https://homes.cs.washington.edu/~mausam/papers/aaai16a.pdf"
  abstract: "We study a novel task of numerical relation extraction with the goal of extracting relations where one of the arguments is a number or a quantity ( e.g., atomic_number(Aluminium, 13), inflation_rate(India, 10.9%)). This task presents peculiar challenges not found in standard IE, such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both systems dramatically outperform MultiR, a state-of-the-art non-numerical IE model, obtaining up to 25 points F-score improvement."
  publication: "https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12486"
  talk: "https://madaan.github.io/res/presentations/aaai-presentation-final.pdf"
  org: AAAI
  org-url: "https://www.aaai.org/"


- title: "Design and implementation of a high performance computing system using distributed compilation"
  authors: "Sunil Kr Singh, Aman Madaan, Ankur Aggarwal, and Ankur Dewan"
  conference: "Advances in Computing, Communications and Informatics (ICACCI)"
  year: 2013
  org: ICACCI
  org-url: http://icacci-conference.org/2018/
  pdf: "https://madaan.github.io/res/papers/distcc-cluster.pdf"
  publication: "https://ieeexplore.ieee.org/document/6637374"
  abstract: "The idea of using the idle resources of a system for some other useful purpose is not new. seti@home pioneered this concept of “public resource computing” by bringing together millions of users worldwide who were ready to donate their idle CPU cycles to the cause of searching the extra terrestrial intelligence. In this paper, we describe a system that uses the same concept for reducing build times by using free cycles on idle computer systems in computer labs of our institute. The challenge of distributing compilation is tackled by a distributing compiler. We use distcc for the purpose. The challenge then is to design a system that keeps track of free helpers, dividing the work fairly among the helpers and most importantly, providing an intuitive interface to the clients who would use the system oblivious of the complexities of the back end."
